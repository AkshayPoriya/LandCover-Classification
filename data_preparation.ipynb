{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preparation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYE3dvcwW9v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZhiLB34cvnJ",
        "colab_type": "text"
      },
      "source": [
        "**Import dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_LkQTWcnf9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io as si         # for inputing matlab files\n",
        "import numpy as np            # Linear Algebra tools\n",
        "from random import shuffle    # for shuffling dataset\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSzyVLL4qe4B",
        "colab_type": "text"
      },
      "source": [
        "**Load Dataset**\n",
        "\n",
        "We have performed our experiments on the [Indian Pines Dataset.](http://www.ehu.eus/ccwintco/index.php?title=Hyperspectral_Remote_Sensing_Scenes)\n",
        "We downloaded MATLAB data files.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2SkmV6oqYQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mat_x = si.loadmat('Indian_pines.mat')['indian_pines']        # shape 145*145*220\n",
        "#mat_y = si.loadmat('Indian_pines_gt.mat')['indian_pines_gt']  # shape 145*145"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et-JLNv5tYyr",
        "colab_type": "text"
      },
      "source": [
        "**Mahesh Sir Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l336hGpwtUkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "# Read csv file\n",
        "x_csv = pd.read_csv(\"/content/data.csv\")\n",
        "y_csv = pd.read_csv(\"/content/ref.csv\")\n",
        "\n",
        "# Convert dataframe to np array\n",
        "x_np = np.array(x_csv)\n",
        "y_np = np.array(y_csv)\n",
        "\n",
        "# Reshape np_array to original picture\n",
        "mat_y = y_np.reshape(330,307)\n",
        "mat_x = np.zeros( (330,307,6) , dtype = np.uint8)\n",
        "\n",
        "# Make band\n",
        "for i in range( x_np.shape[1] ):\n",
        "  band = x_np[:,i]\n",
        "  band = band.reshape(330,307)\n",
        "  mat_x[:,:,i] = band\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnUiQaGk5-PZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten mat_y for further use \n",
        "y = mat_y.flatten('C')   # shape 21025*1     flattened row wise"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgPvxoroMdKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To see enteries of each class\n",
        "#  np.unique(y,return_counts = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhpLo7V2qnEN",
        "colab_type": "text"
      },
      "source": [
        "**Define Global Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brVPYYu4qlvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HEIGHT = mat_x.shape[0]   # 145\n",
        "WIDTH = mat_x.shape[1]    # 145\n",
        "BAND = mat_x.shape[2]     # 220\n",
        "P_S = 5                   # Patch Size\n",
        "OUTPUT_CLASSES = 7       # Total Classes including zero\n",
        "NO_PATCHES = 2000         # No of patches for each class\n",
        "TEST_FRACTION = 0.25      # 75% training_data & 25% testing_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmhB9r00qLwX",
        "colab_type": "text"
      },
      "source": [
        "**Remove class 0**\n",
        "\n",
        "Becoz it has large no of examples as compared to other 16 classes so it tend to overdominate in the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXIMQrXAqJn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(y)):\n",
        "  if(y[i]==0):\n",
        "    y[i]=OUTPUT_CLASSES\n",
        "  else:\n",
        "    y[i] -= 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SombYH-rqZ3",
        "colab_type": "text"
      },
      "source": [
        "**Normalization**\n",
        "\n",
        "Band Max Normalization adopted for Indian Pines DataSet.\n",
        "https://arxiv.org/ftp/arxiv/papers/1710/1710.02939.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQM3U9tirPI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mat_x = mat_x.astype(float)\n",
        "for i in range(BAND):\n",
        "  mat_x[:,:,i] /= np.max(mat_x[:,:,i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N-5tHBzsvfl",
        "colab_type": "text"
      },
      "source": [
        "**Perform the Padding Operation for extracting the patches**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7GxevJerRqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "padding is useful for extracting patches of corner pixels\n",
        "\"\"\"\n",
        "\n",
        "pad_width = int( (P_S-1)/2 )\n",
        "#print(pad_width)\n",
        "padded_x = np.pad(mat_x,[(pad_width,pad_width),(pad_width,pad_width),(0,0)],'constant')\n",
        "#print(padded_x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGwDYtB4vymF",
        "colab_type": "text"
      },
      "source": [
        "**Functions  to extract Patches for each class with label as list index**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SXVythFtH-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to extract patche at h_index,w_index \n",
        "\n",
        "def patch_at_index(h_index,w_index):\n",
        "  patch = padded_x[h_index:h_index+P_S,w_index:w_index+P_S,:]\n",
        "  \"\"\"\n",
        "  we need to convert patch into a vector \n",
        "  \"\"\"\n",
        "  patch = patch.flatten('C')  # Row wise flatten \n",
        "  return patch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXvjamPpyTmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to extract all the patches from hyperspectral image\n",
        "\n",
        "def extract_all_patches():\n",
        "  patches = np.ndarray( shape = (HEIGHT*WIDTH, P_S*P_S*BAND))\n",
        "  for i in range(HEIGHT):\n",
        "    for j in range(WIDTH):\n",
        "      patches[ WIDTH*i + j ] = patch_at_index(i,j)\n",
        "  return patches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0-0RaLC4vQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to split each class patches\n",
        "\n",
        "def split_each_class_patches():\n",
        "  patches = extract_all_patches()\n",
        "  classes = []\n",
        "  for i in range(OUTPUT_CLASSES):\n",
        "    classes.append([])\n",
        "  for i in range(len(patches)):\n",
        "    if( y[i] < OUTPUT_CLASSES ):\n",
        "      classes[y[i]].append(patches[i])\n",
        "  return classes  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lum8bBoK4Cm-",
        "colab_type": "text"
      },
      "source": [
        "**Splitting Training and Testing Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxJAv_5tMw93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to split training and test data\n",
        "\n",
        "def train_test_split():\n",
        "  \n",
        "  classes = split_each_class_patches()\n",
        "  x_train_class, y_train_class, x_test_class, y_test_class = [],[],[],[]    # Each of these lists will be having 17 lists for each class\n",
        "  \n",
        "  for c in range(OUTPUT_CLASSES):\n",
        "    x_train_class.append([])\n",
        "    y_train_class.append([])\n",
        "    x_test_class.append([])\n",
        "    y_test_class.append([])\n",
        "  \n",
        "  for c in range(OUTPUT_CLASSES):\n",
        "    test_set_size = int( len(classes[c]) * TEST_FRACTION )\n",
        "    shuffle(classes[c])\n",
        "    x_test_class[c] += classes[c][0:test_set_size]               # += works becoz both are list, it is exactly same as extend\n",
        "    x_train_class[c] += classes[c][test_set_size:]\n",
        "    y_test_class[c].extend( np.full(test_set_size, c, dtype = int) )   # += dont work here becoz y_test[c] is a list while np.full(..) gives a np array\n",
        "    \n",
        "    temp = x_train_class[c]\n",
        "    #print(len(temp))\n",
        "    for i in range( int( NO_PATCHES / len(x_train_class[c]) ) ):\n",
        "      x_train_class[c]+=temp\n",
        "    \n",
        "    shuffle(x_train_class[c])\n",
        "    x_train_class[c] = x_train_class[c][0:NO_PATCHES]\n",
        "    y_train_class[c].extend( np.full( NO_PATCHES, c, dtype = int) )\n",
        "    \n",
        "  x_train, y_train, x_test, y_test = [],[],[],[]         # actual  data set should be one list or array of samples*feature size\n",
        "  \n",
        "  for c in range(OUTPUT_CLASSES):\n",
        "    x_train.extend( x_train_class[c] )\n",
        "    y_train.extend( y_train_class[c] )\n",
        "    x_test.extend( x_test_class[c] )\n",
        "    y_test.extend( y_test_class[c] )\n",
        "    \n",
        "  \n",
        "  return np.array(x_train), np.array(y_train), np.array(x_test), np.array(y_test) \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}